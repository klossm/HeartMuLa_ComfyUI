import gc
import logging
import os
import sys
import uuid
import warnings

import numpy as np
import soundfile as sf
import torch
import torchaudio
from transformers import BitsAndBytesConfig

import folder_paths

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("torchtune").setLevel(logging.ERROR)
warnings.filterwarnings("ignore")

current_dir = os.path.dirname(os.path.abspath(__file__))
util_dir = os.path.join(current_dir, "util")
if util_dir not in sys.path:
    sys.path.insert(0, util_dir)

folder_paths.add_model_folder_path("HeartMuLa", os.path.join(folder_paths.models_dir, "HeartMuLa"))
folder_paths.add_model_folder_path("HeartMuLa", os.path.join(current_dir, "util", "heartlib", "ckpt"))

def get_model_base_dir():
    paths = folder_paths.get_folder_paths("HeartMuLa")
    for p in paths:
        if os.path.exists(p):
            return p
    return paths[0]

MODEL_BASE_DIR = get_model_base_dir()

def _get_device():
    """Get the best available device: CUDA > MPS > CPU"""
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")

def _get_dtype(device: torch.device):
    """Get the appropriate dtype for the device. MPS has limited bfloat16 support."""
    if device.type == "mps":
        return torch.float32  # MPS bfloat16 causes dtype mismatches in linear layers
    return torch.bfloat16

class HeartMuLaModelManager:
    _instance = None
    _gen_pipes = {}
    _transcribe_pipe = None
    _device = _get_device()

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(HeartMuLaModelManager, cls).__new__(cls)
        return cls._instance

    def get_gen_pipeline(self, version="3B", quantize_4bit=False):
        key = (version, quantize_4bit)
        if key not in self._gen_pipes:
            from heartlib import HeartMuLaGenPipeline

            model_dtype = _get_dtype(self._device)

            bnb_config = None
            if quantize_4bit:
                if self._device.type != "cuda":
                    print(f"HeartMuLa: 4-bit quantization requires CUDA. Ignoring quantize_4bit on {self._device.type}.")
                else:
                    quant_type = "nf4"
                    try:
                        major, _ = torch.cuda.get_device_capability()
                        if major >= 10:
                            quant_type = "fp4"
                            print(f"HeartMuLa: Detected Blackwell GPU (Compute {major}.x), using native FP4 quantization.")
                    except:
                        pass

                    bnb_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_quant_type=quant_type,
                        bnb_4bit_compute_dtype=torch.bfloat16,
                        bnb_4bit_use_double_quant=True,
                    )

            print(f"HeartMuLa: Loading model on {self._device} with dtype {model_dtype}")
            self._gen_pipes[key] = HeartMuLaGenPipeline.from_pretrained(
                MODEL_BASE_DIR,
                device=self._device,
                torch_dtype=model_dtype,
                version=version,
                lazy_load=True,
                bnb_config=bnb_config
            )
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        return self._gen_pipes[key]

    def get_transcribe_pipeline(self):
        if self._transcribe_pipe is None:
            from heartlib import HeartTranscriptorPipeline
            self._transcribe_pipe = HeartTranscriptorPipeline.from_pretrained(
                MODEL_BASE_DIR, device=self._device, dtype=torch.float16,
            )
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
        return self._transcribe_pipe

class HeartMuLa_Generate:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "lyrics": ("STRING", {"multiline": True, "placeholder": "[Verse]\n..."}),
                "tags": ("STRING", {"multiline": True, "placeholder": "piano,happy,wedding"}),
                "version": (["3B", "7B"], {"default": "3B"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}),
                "max_audio_length_seconds": ("INT", {"default": 240, "min": 10, "max": 600, "step": 1}),
                "topk": ("INT", {"default": 50, "min": 1, "max": 250, "step": 1}),
                "temperature": ("FLOAT", {"default": 1.0, "min": 0.1, "max": 2.0, "step": 0.01}),
                "cfg_scale": ("FLOAT", {"default": 1.5, "min": 1.0, "max": 10.0, "step": 0.1}),
                "keep_model_loaded": ("BOOLEAN", {"default": True}),
                "offload_mode": (["auto", "aggressive"], {"default": "auto"}),
                "quantize_4bit": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("AUDIO", "STRING")
    RETURN_NAMES = ("audio_output", "filepath")
    FUNCTION = "generate"
    CATEGORY = "HeartMuLa"

    def generate(self, lyrics, tags, version, seed, max_audio_length_seconds, topk, temperature, cfg_scale, keep_model_loaded, offload_mode="auto", quantize_4bit=False):
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
        np.random.seed(seed & 0xFFFFFFFF)

        max_audio_length_ms = int(max_audio_length_seconds * 1000)

        manager = HeartMuLaModelManager()
        pipe = manager.get_gen_pipeline(version, quantize_4bit=quantize_4bit)

        output_dir = folder_paths.get_output_directory()
        os.makedirs(output_dir, exist_ok=True)
        filename = f"heartmula_gen_{uuid.uuid4().hex}.wav"
        out_path = os.path.join(output_dir, filename)

        try:
            with torch.inference_mode():
                pipe(
                    {"lyrics": lyrics, "tags": tags},
                    max_audio_length_ms=max_audio_length_ms,
                    save_path=out_path,
                    topk=topk,
                    temperature=temperature,
                    cfg_scale=cfg_scale,
                    keep_model_loaded=keep_model_loaded,
                    offload_mode=offload_mode
                )
        except Exception as e:
            print(f"Generation failed: {e}")
            raise e
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

        try:
            waveform, sample_rate = torchaudio.load(out_path)
            waveform = waveform.float()
            if waveform.ndim == 2:
                waveform = waveform.unsqueeze(0)
        except Exception:
            waveform_np, sample_rate = sf.read(out_path)
            if waveform_np.ndim == 1:
                waveform_np = waveform_np[np.newaxis, :]
            else:
                waveform_np = waveform_np.T
            waveform = torch.from_numpy(waveform_np).float()
            if waveform.ndim == 2:
                waveform = waveform.unsqueeze(0)

        audio_output = {"waveform": waveform, "sample_rate": sample_rate}
        return (audio_output, out_path)

class HeartMuLa_Transcribe:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_input": ("AUDIO",),
                "temperature_tuple": ("STRING", {"default": "0.0,0.1,0.2,0.4"}),
                "no_speech_threshold": ("FLOAT", {"default": 0.4, "min": 0.0, "max": 1.0, "step": 0.05}),
                "logprob_threshold": ("FLOAT", {"default": -1.0, "min": -5.0, "max": 5.0, "step": 0.1}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("lyrics_text",)
    FUNCTION = "transcribe"
    CATEGORY = "HeartMuLa"

    def transcribe(self, audio_input, temperature_tuple, no_speech_threshold, logprob_threshold):
        if isinstance(audio_input, dict):
            waveform = audio_input["waveform"]
            sr = audio_input["sample_rate"]
        else:
            sr, waveform = audio_input
            if isinstance(waveform, np.ndarray):
                 waveform = torch.from_numpy(waveform)

        if waveform.ndim == 3:
            waveform = waveform.squeeze(0)
        elif waveform.ndim == 1:
            waveform = waveform.unsqueeze(0)

        waveform = waveform.to(torch.float32).cpu()
        output_dir = folder_paths.get_temp_directory()
        os.makedirs(output_dir, exist_ok=True)
        temp_path = os.path.join(output_dir, f"hm_trans_{uuid.uuid4().hex}.wav")

        wav_np = waveform.numpy()
        if wav_np.ndim == 2:
            wav_np = wav_np.T
        sf.write(temp_path, wav_np, sr)

        try:
            temp_tuple = tuple(float(x.strip()) for x in temperature_tuple.split(","))
        except:
            temp_tuple = (0.0, 0.1, 0.2, 0.4)

        manager = HeartMuLaModelManager()
        pipe = manager.get_transcribe_pipeline()

        try:
            with torch.inference_mode():
                result = pipe(
                    temp_path,
                    temperature=temp_tuple,
                    no_speech_threshold=no_speech_threshold,
                    logprob_threshold=logprob_threshold,
                    task="transcribe",
                )
        finally:
            if os.path.exists(temp_path):
                os.remove(temp_path)
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

        text = result if isinstance(result, str) else result.get("text", str(result))
        return (text,)

NODE_CLASS_MAPPINGS = {
    "HeartMuLa_Generate": HeartMuLa_Generate,
    "HeartMuLa_Transcribe": HeartMuLa_Transcribe,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "HeartMuLa_Generate": "HeartMuLa Music Generator",
    "HeartMuLa_Transcribe": "HeartMuLa Lyrics Transcriber",
}


__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']
